{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "data_path = './data/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "cifar10_validation = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "label_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, download=True, transform=train_transform)\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=True, transform=validation_transform)\n",
    "# cifar10_val_train = datasets.CIFAR10(data_path, train=True, download=True, transform=validation_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlockPlain(nn.Module):\n",
    "    def __init__(self, input_channels, channel_factor=1, stride=1):\n",
    "        super().__init__()\n",
    "        self.channel_factor = channel_factor\n",
    "        self.stride = stride\n",
    "\n",
    "        out_channels = input_channels * channel_factor\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3, padding=1, stride=stride,\n",
    "            bias=False\n",
    "        )\n",
    "        self.batch1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3, padding=1, stride=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.batch2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #     elif isinstance(m, nn.BatchNorm2d):\n",
    "        #         nn.init.constant_(m.weight, 1)\n",
    "        #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # if stride != 1:\n",
    "        #     self.downsample = nn.MaxPool2d(stride,stride)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if channel_factor != 1 or stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, input_channels * channel_factor, kernel_size=1, padding=0, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(input_channels * channel_factor)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch2(out)\n",
    "        # if self.stride != 1 or self.channel_factor != 1:\n",
    "        # #     x = self.downsample(x)\n",
    "        # # if self.channel_factor != 1:\n",
    "        #     x = self.cut(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    # def shortcut(self, input_channels, output_channels, stride):\n",
    "    #     return nn.Conv2d(input_channels, output_channels, kernel_size=1, padding=0, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.resblocksm1 = nn.Sequential(\n",
    "            *[ResBlockPlain(64) for _ in range(2)]\n",
    "        )\n",
    "        self.resblocks0 = ResBlockPlain(64, 2, stride=2)\n",
    "        self.resblocks1 = nn.Sequential(\n",
    "            *[ResBlockPlain(128) for _ in range(1)]\n",
    "        )\n",
    "        self.resblocks2 = ResBlockPlain(128, 2, stride=2)\n",
    "        self.resblocks3 = nn.Sequential(\n",
    "            *[ResBlockPlain(256, 1) for _ in range(1)]\n",
    "        )\n",
    "        self.resblocks4 = ResBlockPlain(256, 2, stride=2)\n",
    "        self.resblocks5 = nn.Sequential(\n",
    "            *[ResBlockPlain(512, 1) for _ in range(1)]\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        # out = self.pool1(out)\n",
    "        # out = F.max_pool2d(out, 2)\n",
    "        out = self.resblocksm1(out)\n",
    "        out = self.resblocks0(out)\n",
    "        out = self.resblocks1(out)\n",
    "        out = self.resblocks2(out)\n",
    "        out = self.resblocks3(out)\n",
    "        out = self.resblocks4(out)\n",
    "        out = self.resblocks5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out,1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetResDeep(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (resblocksm1): Sequential(\n",
       "    (0): ResBlockPlain(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlockPlain(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblocks0): ResBlockPlain(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (cut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (resblocks1): Sequential(\n",
       "    (0): ResBlockPlain(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlockPlain(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblocks2): ResBlockPlain(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (cut): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (resblocks3): Sequential(\n",
       "    (0): ResBlockPlain(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlockPlain(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblocks4): ResBlockPlain(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (cut): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (resblocks5): Sequential(\n",
       "    (0): ResBlockPlain(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResBlockPlain(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NetResDeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs: int, optimizer, scheduler, model, loss_fn, train_loader, validate_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            train_acc, test_acc = validate(model, train_loader, validate_loader)\n",
    "            print(f\"{datetime.datetime.now()} {epoch}/{n_epochs}: loss {loss_train / len(train_loader):.4f}, acc = ({train_acc:.3f},{test_acc:.3f})\")\n",
    "\n",
    "def validate(model, train_loader, validate_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", validate_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels\n",
    "                # outputs = validate_tta(imgs, tta_layer, model, size=9).cpu()\n",
    "                outputs = model(imgs).cpu()\n",
    "                predicted = np.argmax(outputs.numpy(), axis=1)\n",
    "                total += labels.shape[0]\n",
    "\n",
    "                correct += int((predicted == labels.numpy()).sum())\n",
    "\n",
    "            result.append(correct / total)\n",
    "            # print(f\"{name} acc: {correct / total:.3f}\")\n",
    "    model.train()\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar10_expanded[2]\n",
    "img = img.permute(1,2,0)\n",
    "#print(img.shape)\n",
    "#print(img)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 32]) torch.Size([16])\n",
      "tensor([4, 5, 5, 7, 8, 5, 0, 2, 7, 7, 2, 1, 7, 3, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=16, shuffle=True)\n",
    "for imgs, labels in train_loader:\n",
    "    print(imgs.shape, labels.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "2022-05-20 09:28:48.164180 1/140: loss 1.8458, acc = (0.406,0.432)\n",
      "2022-05-20 09:33:14.285354 10/140: loss 0.4919, acc = (0.798,0.789)\n",
      "2022-05-20 09:38:08.853258 20/140: loss 0.3830, acc = (0.853,0.834)\n",
      "2022-05-20 09:43:05.044361 30/140: loss 0.3426, acc = (0.829,0.808)\n",
      "2022-05-20 09:48:00.156343 40/140: loss 0.3238, acc = (0.863,0.844)\n",
      "2022-05-20 09:52:59.810683 50/140: loss 0.2958, acc = (0.844,0.822)\n",
      "2022-05-20 09:57:53.766002 60/140: loss 0.2800, acc = (0.897,0.871)\n",
      "2022-05-20 10:02:47.729432 70/140: loss 0.2623, acc = (0.907,0.880)\n",
      "2022-05-20 10:07:41.922167 80/140: loss 0.2398, acc = (0.903,0.869)\n",
      "2022-05-20 10:12:37.905123 90/140: loss 0.2177, acc = (0.906,0.879)\n",
      "2022-05-20 10:17:31.838376 100/140: loss 0.1934, acc = (0.936,0.899)\n",
      "2022-05-20 10:22:26.724729 110/140: loss 0.1621, acc = (0.939,0.902)\n",
      "2022-05-20 10:27:21.018228 120/140: loss 0.1347, acc = (0.940,0.896)\n",
      "2022-05-20 10:32:14.876872 130/140: loss 0.1103, acc = (0.966,0.920)\n",
      "2022-05-20 10:37:10.043448 140/140: loss 0.0761, acc = (0.970,0.916)\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=128, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=128, shuffle=False)\n",
    "model = NetResDeep()\n",
    "# model.load_state_dict(torch.load(data_path + \"cifar10_2.pt\"))\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "# model.load_state_dict(torch.load(data_path + \"cifar10.pt\"))\n",
    "model = model.to(device=device)\n",
    "\n",
    "print(device)\n",
    "# model = model.to(device)\n",
    "# model = Net().to(device=device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=140,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    validate_loader=validate_loader\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), data_path + \"cifar10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = loaded_model.state_dict()\n",
    "pretrained_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_dict.items()}\n",
    "m_model = NetResDeep().to(device=device)\n",
    "m_model.load_state_dict(pretrained_dict)\n",
    "torch.save(m_model.state_dict(), data_path + \"cifar10_mm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): NetResDeep(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (resblocksm1): Sequential(\n",
       "      (0): ResBlockPlain(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResBlockPlain(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (resblocks0): ResBlockPlain(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (resblocks1): Sequential(\n",
       "      (0): ResBlockPlain(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResBlockPlain(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (resblocks2): ResBlockPlain(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cut): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (resblocks3): Sequential(\n",
       "      (0): ResBlockPlain(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResBlockPlain(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (resblocks4): ResBlockPlain(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cut): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (resblocks5): Sequential(\n",
       "      (0): ResBlockPlain(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ResBlockPlain(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (batch2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = NetResDeep().to(device=device)\n",
    "loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2])\n",
    "loaded_model.load_state_dict(torch.load(data_path + \"cifar10.pt\"))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -14.9913,  -53.5412, -114.9717,   32.8586,  -42.5173,   26.1958,\n",
      "          -59.7509, -132.1227,  -20.0103,  -10.1331]], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>) 3\n"
     ]
    }
   ],
   "source": [
    "def validate_tta(imgs, tta_layer, model, size=9):\n",
    "    # imgs -> N, C, H, W\n",
    "    augmented = torch.vstack([*[tta_layer(imgs) for _ in range(size-1)], imgs])\n",
    "    # augmented -> 9, N, C, H, W -> 9*N, C, H, W\n",
    "    augmented = augmented.view(-1, imgs.shape[1], imgs.shape[2], imgs.shape[3])\n",
    "    predicted = model(augmented.to(device=device))\n",
    "    predicted = predicted.view(size, imgs.shape[0], 10)\n",
    "    return torch.sum(predicted, 0)\n",
    "\n",
    "\n",
    "img, l = cifar10[9]\n",
    "# plt.imshow(img.permute(1,2,0))\n",
    "# plt.show()\n",
    "# predicted = loaded_model(img.unsqueeze(0).to(device=device))\n",
    "\n",
    "tta_layer = nn.Sequential(\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    ")\n",
    "\n",
    "loaded_model.eval()\n",
    "predicted = validate_tta(img.unsqueeze(0), tta_layer, loaded_model)\n",
    "loaded_model.train()\n",
    "print(predicted, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.986\n",
      "val acc: 0.876\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10_val_train, batch_size=256, shuffle=False)\n",
    "validate_loader = torch.utils.data.DataLoader(cifar10_validation, batch_size=1024, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, validate_loader):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", validate_loader)]:\n",
    "        if loader is None:\n",
    "            continue\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels\n",
    "                if name == \"val\":\n",
    "                    outputs = validate_tta(imgs, tta_layer, model, size=9).cpu()\n",
    "                else:\n",
    "                    outputs = model(imgs).cpu()\n",
    "                predicted = np.argmax(outputs.numpy(), axis=1)\n",
    "                total += labels.shape[0]\n",
    "\n",
    "                correct += int((predicted == labels.numpy()).sum())\n",
    "\n",
    "            print(f\"{name} acc: {correct / total:.3f}\")\n",
    "    model.train()\n",
    "\n",
    "# def validate(model, train_loader, validate_loader):\n",
    "#     model.eval()\n",
    "#     result = []\n",
    "#     for name, loader in [(\"train\", train_loader), (\"val\", validate_loader)]:\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for imgs, labels in loader:\n",
    "#                 imgs = imgs.to(device=device)\n",
    "#                 labels = labels\n",
    "#                 outputs = model(imgs).cpu()\n",
    "#                 predicted = np.argmax(outputs.numpy(), axis=1)\n",
    "#                 total += labels.shape[0]\n",
    "\n",
    "#                 correct += int((predicted == labels.numpy()).sum())\n",
    "\n",
    "#             result.append(correct / total)\n",
    "#             # print(f\"{name} acc: {correct / total:.3f}\")\n",
    "#     model.train()\n",
    "#     return tuple(result)\n",
    "\n",
    "validate(loaded_model, train_loader, validate_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs: int, optimizer, scheduler, model, loss_fn, train_loader, validate_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            train_acc, test_acc = validate(model, train_loader, validate_loader)\n",
    "            print(f\"{datetime.datetime.now()} {epoch}/{n_epochs}: loss {loss_train / len(train_loader):.4f}, acc = ({train_acc:.3f},{test_acc:.3f})\")\n",
    "\n",
    "def validate(model, train_loader, validate_loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", validate_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels\n",
    "                outputs = model(imgs).cpu()\n",
    "                predicted = np.argmax(outputs.numpy(), axis=1)\n",
    "                total += labels.shape[0]\n",
    "\n",
    "                correct += int((predicted == labels.numpy()).sum())\n",
    "\n",
    "            result.append(correct / total)\n",
    "            # print(f\"{name} acc: {correct / total:.3f}\")\n",
    "    model.train()\n",
    "    return tuple(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
